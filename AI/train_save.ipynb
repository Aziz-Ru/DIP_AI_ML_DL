{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001e39da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 13:31:37.370206: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-22 13:31:37.371096: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-22 13:31:37.376183: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-22 13:31:37.399922: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758526297.435276   16580 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758526297.444913   16580 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1758526297.470126   16580 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758526297.470166   16580 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758526297.470171   16580 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758526297.470175   16580 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-22 13:31:37.478843: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/silicon/opencv_learn/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-09-22 13:31:42.615111: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                </span>┃<span style=\"font-weight: bold\"> Output Shape          </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Trai… </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ max_pooling2d               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │   <span style=\"font-weight: bold\">-</span>   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)              │                       │            │       │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1352</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │   <span style=\"font-weight: bold\">-</span>   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">173,184</span> │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">170</span> │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "└─────────────────────────────┴───────────────────────┴────────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTrai…\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m8\u001b[0m)     │         \u001b[38;5;34m80\u001b[0m │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ max_pooling2d               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m8\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │   \u001b[1m-\u001b[0m   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)              │                       │            │       │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1352\u001b[0m)          │          \u001b[38;5;34m0\u001b[0m │   \u001b[1m-\u001b[0m   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │    \u001b[38;5;34m173,184\u001b[0m │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │      \u001b[38;5;34m8,256\u001b[0m │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │      \u001b[38;5;34m2,080\u001b[0m │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)            │        \u001b[38;5;34m528\u001b[0m │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "├─────────────────────────────┼───────────────────────┼────────────┼───────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)            │        \u001b[38;5;34m170\u001b[0m │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "└─────────────────────────────┴───────────────────────┴────────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">184,298</span> (719.91 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m184,298\u001b[0m (719.91 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">184,298</span> (719.91 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m184,298\u001b[0m (719.91 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8036 - loss: 0.5872"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: models/experiment_1/model_epoch_1.h5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.8037 - loss: 0.5870 - val_accuracy: 0.9682 - val_loss: 0.1128\n",
      "Epoch 2/10\n",
      "\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9721 - loss: 0.0899"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: models/experiment_1/model_epoch_2.h5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9721 - loss: 0.0899 - val_accuracy: 0.9741 - val_loss: 0.0875\n",
      "Epoch 3/10\n",
      "\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9826 - loss: 0.0577"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: models/experiment_1/model_epoch_3.h5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9826 - loss: 0.0577 - val_accuracy: 0.9800 - val_loss: 0.0685\n",
      "Epoch 4/10\n",
      "\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9879 - loss: 0.0391"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: models/experiment_1/model_epoch_4.h5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9879 - loss: 0.0391 - val_accuracy: 0.9800 - val_loss: 0.0722\n",
      "Epoch 5/10\n",
      "\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9915 - loss: 0.0272"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: models/experiment_1/model_epoch_5.h5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9915 - loss: 0.0272 - val_accuracy: 0.9828 - val_loss: 0.0636\n",
      "Epoch 6/10\n",
      "\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9930 - loss: 0.0220"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: models/experiment_1/model_epoch_6.h5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9930 - loss: 0.0220 - val_accuracy: 0.9831 - val_loss: 0.0625\n",
      "Epoch 7/10\n",
      "\u001b[1m1494/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9954 - loss: 0.0147"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: models/experiment_1/model_epoch_7.h5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.9954 - loss: 0.0147 - val_accuracy: 0.9803 - val_loss: 0.0763\n",
      "Epoch 8/10\n",
      "\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9952 - loss: 0.0149"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: models/experiment_1/model_epoch_8.h5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 9ms/step - accuracy: 0.9952 - loss: 0.0149 - val_accuracy: 0.9842 - val_loss: 0.0685\n",
      "Epoch 9/10\n",
      "\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9964 - loss: 0.0109"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: models/experiment_1/model_epoch_9.h5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 9ms/step - accuracy: 0.9964 - loss: 0.0109 - val_accuracy: 0.9807 - val_loss: 0.0836\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9970 - loss: 0.0093"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: models/experiment_1/model_epoch_10.h5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9970 - loss: 0.0093 - val_accuracy: 0.9837 - val_loss: 0.0759\n",
      "\n",
      "Experiment 1 completed!\n",
      "Files saved:\n",
      "- Models: models/experiment_1/\n",
      "- Training log: experiment_logs/train_experiment_1.csv\n",
      "- Test log: experiment_logs/test_experiment_1.csv\n",
      "- Summary: experiment_logs/experiment_1_summary.json\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.callbacks import Callback\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ExperimentTracker(Callback):\n",
    "    \"\"\"Custom callback to track and save experiment data after each epoch\"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_no, x_test=None, y_test=None, save_model=True):\n",
    "        super().__init__()\n",
    "        self.experiment_no = experiment_no\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.save_model = save_model\n",
    "        \n",
    "        # Initialize tracking lists\n",
    "        self.train_data = []\n",
    "        self.test_data = []\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(f'models/experiment_{experiment_no}', exist_ok=True)\n",
    "        os.makedirs('experiment_logs', exist_ok=True)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"Save model and log data after each epoch\"\"\"\n",
    "        logs = logs or {}\n",
    "        \n",
    "        # Save model after each epoch\n",
    "        if self.save_model:\n",
    "            model_path = f'models/experiment_{self.experiment_no}/model_epoch_{epoch+1}.h5'\n",
    "            self.model.save(model_path)\n",
    "            print(f\"Model saved: {model_path}\")\n",
    "        \n",
    "        # Prepare training data for CSV\n",
    "        train_row = {\n",
    "            'experiment_no': self.experiment_no,\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': logs.get('loss', 0),\n",
    "            'train_accuracy': logs.get('accuracy', 0),\n",
    "            'val_loss': logs.get('val_loss', 0),\n",
    "            'val_accuracy': logs.get('val_accuracy', 0),\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        self.train_data.append(train_row)\n",
    "        \n",
    "        # Evaluate on test set if provided\n",
    "        if self.x_test is not None and self.y_test is not None:\n",
    "            test_loss, test_accuracy = self.model.evaluate(self.x_test, self.y_test, verbose=0)\n",
    "            test_row = {\n",
    "                'experiment_no': self.experiment_no,\n",
    "                'epoch': epoch + 1,\n",
    "                'test_loss': test_loss,\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            self.test_data.append(test_row)\n",
    "        \n",
    "        # Save/update CSV files after each epoch\n",
    "        self._save_csv_files()\n",
    "    \n",
    "    def _save_csv_files(self):\n",
    "        \"\"\"Save training and test data to CSV files\"\"\"\n",
    "        # Save training experiment data\n",
    "        train_df = pd.DataFrame(self.train_data)\n",
    "        train_csv_path = f'experiment_logs/train_experiment_{self.experiment_no}.csv'\n",
    "        train_df.to_csv(train_csv_path, index=False)\n",
    "        \n",
    "        # Save test experiment data if available\n",
    "        if self.test_data:\n",
    "            test_df = pd.DataFrame(self.test_data)\n",
    "            test_csv_path = f'experiment_logs/test_experiment_{self.experiment_no}.csv'\n",
    "            test_df.to_csv(test_csv_path, index=False)\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        \"\"\"Save final experiment summary\"\"\"\n",
    "        summary = {\n",
    "            'experiment_no': self.experiment_no,\n",
    "            'total_epochs': len(self.train_data),\n",
    "            'final_train_loss': self.train_data[-1]['train_loss'],\n",
    "            'final_train_accuracy': self.train_data[-1]['train_accuracy'],\n",
    "            'final_val_loss': self.train_data[-1]['val_loss'],\n",
    "            'final_val_accuracy': self.train_data[-1]['val_accuracy'],\n",
    "            'best_val_accuracy': max([row['val_accuracy'] for row in self.train_data]),\n",
    "            'training_completed': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        if self.test_data:\n",
    "            summary['final_test_loss'] = self.test_data[-1]['test_loss']\n",
    "            summary['final_test_accuracy'] = self.test_data[-1]['test_accuracy']\n",
    "        \n",
    "        # Save summary as JSON\n",
    "        with open(f'experiment_logs/experiment_{self.experiment_no}_summary.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "\n",
    "# Example usage:\n",
    "def run_experiment(experiment_no, x_train, y_train, x_val, y_val, x_test=None, y_test=None):\n",
    "    \"\"\"Run a complete experiment with tracking\"\"\"\n",
    "    \n",
    "    # Create your model\n",
    "    model = Sequential([\n",
    "        Conv2D(filters=8, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Show model summary\n",
    "    model.summary(show_trainable=True)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Create experiment tracker callback\n",
    "    tracker = ExperimentTracker(\n",
    "        experiment_no=experiment_no,\n",
    "        x_test=x_test,\n",
    "        y_test=y_test,\n",
    "        save_model=True\n",
    "    )\n",
    "    \n",
    "    # Train model with tracking\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_data=(x_val, y_val),\n",
    "        callbacks=[tracker],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Example with MNIST data:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data (example with MNIST)\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    \n",
    "    # Normalize and reshape\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "    \n",
    "    # Create validation split\n",
    "    split_idx = int(0.8 * len(x_train))\n",
    "    x_val = x_train[split_idx:]\n",
    "    y_val = y_train[split_idx:]\n",
    "    x_train = x_train[:split_idx]\n",
    "    y_train = y_train[:split_idx]\n",
    "    \n",
    "    # Run experiment\n",
    "    experiment_number = 1\n",
    "    model, history = run_experiment(\n",
    "        experiment_no=experiment_number,\n",
    "        x_train=x_train,\n",
    "        y_train=y_train,\n",
    "        x_val=x_val,\n",
    "        y_val=y_val,\n",
    "        x_test=x_test,\n",
    "        y_test=y_test\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nExperiment {experiment_number} completed!\")\n",
    "    print(f\"Files saved:\")\n",
    "    print(f\"- Models: models/experiment_{experiment_number}/\")\n",
    "    print(f\"- Training log: experiment_logs/train_experiment_{experiment_number}.csv\")\n",
    "    print(f\"- Test log: experiment_logs/test_experiment_{experiment_number}.csv\")\n",
    "    print(f\"- Summary: experiment_logs/experiment_{experiment_number}_summary.json\")\n",
    "\n",
    "# Additional utility functions:\n",
    "def load_experiment_data(experiment_no):\n",
    "    \"\"\"Load experiment data from CSV files\"\"\"\n",
    "    train_df = pd.read_csv(f'experiment_logs/train_experiment_{experiment_no}.csv')\n",
    "    \n",
    "    try:\n",
    "        test_df = pd.read_csv(f'experiment_logs/test_experiment_{experiment_no}.csv')\n",
    "        return train_df, test_df\n",
    "    except FileNotFoundError:\n",
    "        return train_df, None\n",
    "\n",
    "def compare_experiments(experiment_numbers):\n",
    "    \"\"\"Compare multiple experiments\"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    for exp_no in experiment_numbers:\n",
    "        try:\n",
    "            with open(f'experiment_logs/experiment_{exp_no}_summary.json', 'r') as f:\n",
    "                summary = json.load(f)\n",
    "                comparison_data.append(summary)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Summary for experiment {exp_no} not found\")\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "# Example of comparing experiments:\n",
    "# comparison_df = compare_experiments([1, 2, 3])\n",
    "# print(comparison_df[['experiment_no', 'final_val_accuracy', 'best_val_accuracy']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv_learn (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
